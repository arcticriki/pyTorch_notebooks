{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Static\" declaration vs \"Dynamic\" declaration\n",
    "\n",
    "The majority of DL frameworks use a \"static\" declaration paradigm for the definition of the computational graph. This programming paradigm requires programmers to define the network architecture once using symbolic expressions before beginning execution (running the session). Then, for a given graph and data samples, the software toolkits can automatically derive the correct algorithm for training or inference following backpropagation and auto-differentiation rules. This procedure can be described with the following pseudo-code:\n",
    "\n",
    "<img src=\"files/static.png\" width=\"600\">\n",
    "\n",
    "- execution naturally batched\n",
    "- improved parallelization\n",
    "- optimization of the graph at declaration time\n",
    "\n",
    "However, in some fields we could need a dynamic NN. This dynamicity can come from multiple dimensions, like: variably sized I/O variably structured I/O, non-trivial inference algorithms. Consider this example:\n",
    "\n",
    "<img src=\"files/dybamic_example.png\" width=\"400\">\n",
    "\n",
    "\n",
    "The above figure shows an example of a network that takes into account this syntactic structure, generating representations for the sentence by traversing the parse tree bottom-up and combining the representations for each sub-tree using a dynamic NN called **Tree Structured Long Short-term Memory (Tree-LSTM)**. Each node of the tree maps to a LSTM function. Each node takes a variable number of inputs and returns a vector representing the parsing semantics up to that point back to the leaf node. This goes on until the root LSTM node returns a vector representing the semantics of the entire sentence.\n",
    "\n",
    "**Itâ€™s important to observe that the NN structure varies with the underlying parsing tree over each input sample, but the same LSTM cell (i.e. the parametrization point of the model) is constant in shape and repeated at each internal node.**\n",
    "\n",
    "Such architecture cannot be defined in a static way. Every possible sample could need a different graph. We need the dynamic declaration:\n",
    "\n",
    "<img src=\"files/dynamic.png\" width=\"600\">\n",
    "\n",
    "- batching now is not \"natural\"\n",
    "- parallelization can be harder. In the wrost case, you must use BATCH_SIZE = 1\n",
    "- more difficult to debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyTorch Dynamic declaration\n",
    "\n",
    "We have seen that the computational graph is built in the forward pass of the NN class.\n",
    "Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model. All these are perfectly legal, and will be handled correctly by autograd.\n",
    "\n",
    "To showcase the power of PyTorch dynamic graphs, we will implement a very strange model: a fully-connected ReLU network that on each forward pass randomly chooses a number between 1 and 4 and has that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers. We will use this for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299703\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.920332\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.031918\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.499797\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.933040\n",
      "\n",
      "Test set: Average loss: 0.5941, Accuracy: 8227/10000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.604599\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.464041\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.380010\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.385107\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.244151\n",
      "\n",
      "Test set: Average loss: 0.3436, Accuracy: 8998/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.287145\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.287800\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.370325\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.358409\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.179007\n",
      "\n",
      "Test set: Average loss: 0.2839, Accuracy: 9195/10000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.276954\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.203615\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.289445\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.191462\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.336434\n",
      "\n",
      "Test set: Average loss: 0.2371, Accuracy: 9305/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.294001\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.271237\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.236239\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.103684\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.195655\n",
      "\n",
      "Test set: Average loss: 0.2311, Accuracy: 9328/10000 (93%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.307661\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.266926\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.180318\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.164970\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.137740\n",
      "\n",
      "Test set: Average loss: 0.1874, Accuracy: 9452/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.162652\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.131607\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.160980\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.261387\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.158401\n",
      "\n",
      "Test set: Average loss: 0.1711, Accuracy: 9492/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.133459\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.168099\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.193453\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.094900\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.283868\n",
      "\n",
      "Test set: Average loss: 0.1728, Accuracy: 9496/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import sigmoid, relu\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1000\n",
    "EPOCHS = 8\n",
    "LOG_INTERVAL = 100   # how many batches to wait before logging training status\n",
    "\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = relu(self.input_linear(x))\n",
    "        coin = random.randint(0, 3)   # throw a coin to choose {0,1,2,3}\n",
    "        for _ in range(coin):         # add \"coin\" number of layers \n",
    "            x = relu(self.middle_linear(x))\n",
    "        x = relu(self.output_linear(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, liveloss):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        liveloss.update({'loss': loss.item()})\n",
    "        #liveloss.draw()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    seed = 666\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    # Download datasets\n",
    "    train_MNIST = datasets.MNIST('../data', train=True, download=True,\n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    test_MNIST =  datasets.MNIST('../data', train=False,\n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    # Fork into the DataLoader object\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_MNIST,batch_size=TRAIN_BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_MNIST, batch_size=TEST_BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "    # Define the model (remember to assign it to cuda)\n",
    "    D_in, H, D_out = 784, 100, 10\n",
    "    model = DynamicNet(D_in, H, D_out).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    \n",
    "    liveloss_train = PlotLosses()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, liveloss_train)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example can be the following: let's throw a coin and create a \"tree like\" structure randomly, varying both in the number of layers and neurons per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.609661\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.207432\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.833328\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.707430\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.534666\n",
      "\n",
      "Test set: Average loss: 0.7379, Accuracy: 8389/10000 (84%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.008146\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.091399\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.389304\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.088716\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.421001\n",
      "\n",
      "Test set: Average loss: 0.7323, Accuracy: 8476/10000 (85%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.362246\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.827205\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.999241\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.924134\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.354726\n",
      "\n",
      "Test set: Average loss: 0.4656, Accuracy: 8864/10000 (89%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.318967\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.838309\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.258139\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.055566\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.213149\n",
      "\n",
      "Test set: Average loss: 0.6679, Accuracy: 8495/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.336005\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.265331\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.671391\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.251269\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.297042\n",
      "\n",
      "Test set: Average loss: 0.6558, Accuracy: 8559/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.661277\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.610208\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.836260\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.263297\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.595034\n",
      "\n",
      "Test set: Average loss: 0.5252, Accuracy: 8800/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.134305\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.220581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-34:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/datasets/mnist.py\", line 77, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/functional.py\", line 70, in to_tensor\n",
      "    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/PIL/Image.py\", line 718, in tobytes\n",
      "    self.load()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/PIL/Image.py\", line 824, in load\n",
      "    return self.im.pixel_access(self.readonly)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 376, in _fixed_getinnerframes\n",
      "    lines = ulinecache.getlines(file)[start:end]\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/utils/ulinecache.py\", line 37, in getlines\n",
      "    return [l.decode(encoding, 'replace') for l in lines]\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 11671) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import sigmoid, relu\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1000\n",
    "EPOCHS = 8\n",
    "LOG_INTERVAL = 100   # how many batches to wait before logging training status\n",
    "\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H1)\n",
    "        self.middle_linear1 = torch.nn.Linear(H1, H2)\n",
    "        self.middle_linear2 = torch.nn.Linear(H2, H1)\n",
    "        self.middle_linear3 = torch.nn.Linear(H1, H1)\n",
    "        self.output_linear = torch.nn.Linear(H1, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = relu(self.input_linear(x))\n",
    "        \n",
    "        coin = random.randint(0, 3)\n",
    "        if coin == 1:\n",
    "            x = relu(self.middle_linear1(x))\n",
    "        elif coin == 2:\n",
    "            x = relu(self.middle_linear1(x))\n",
    "            x = relu(self.middle_linear2(x))\n",
    "        elif coin == 3:\n",
    "            x = relu(self.middle_linear1(x))\n",
    "            x = relu(self.middle_linear2(x))\n",
    "            x = relu(self.middle_linear3(x))\n",
    "        else:\n",
    "            x = relu(self.output_linear(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, liveloss):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        liveloss.update({'loss': loss.item()})\n",
    "        #liveloss.draw()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    seed = 666\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    # Download datasets\n",
    "    train_MNIST = datasets.MNIST('../data', train=True, download=True,\n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    test_MNIST =  datasets.MNIST('../data', train=False,\n",
    "                                 transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    # Fork into the DataLoader object\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_MNIST,batch_size=TRAIN_BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_MNIST, batch_size=TEST_BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "    # Define the model (remember to assign it to cuda)\n",
    "    D_in, H1, H2, D_out = 784, 100, 300, 10\n",
    "    model = DynamicNet(D_in, H1, H2, D_out).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    \n",
    "    liveloss_train = PlotLosses()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, liveloss_train)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, in your model definition you can go full crazy and use arbitrary python code to define your model structure. Conditional statements can be done even evaluating some input tensor propriety, like:\n",
    "\n",
    "```\n",
    " while x.norm(2) < 10:\n",
    "     x = self.conv1(x)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
